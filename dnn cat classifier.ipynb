{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning from scratch\n",
    "## Deep Neural Networks for a cat classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import time as tm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Pre-processing images\n",
    "Skip this step after the first time running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dir = 'cat/'\n",
    "not_cat_dir = 'not-cat/'\n",
    "wolf_dir = 'wolf/'\n",
    "\n",
    "cat_files = [f for f in listdir(cat_dir) if isfile(join(cat_dir, f))]\n",
    "not_cat_files = [f for f in listdir(not_cat_dir) if isfile(join(not_cat_dir, f))]\n",
    "wolf_files = [f for f in listdir(wolf_dir) if isfile(join(wolf_dir, f))]\n",
    "\n",
    "width = 128\n",
    "height = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = []\n",
    "for filename in cat_files:\n",
    "    with Image.open(cat_dir+filename) as im:\n",
    "        if im.mode == 'RGB':\n",
    "            nim = im.resize((width, height), Image.BILINEAR)\n",
    "            pixel_values = np.array(nim.getdata()).reshape((width, height, 3))\n",
    "            x_cat.append(pixel_values)\n",
    "x_cat_array = np.array(x_cat)\n",
    "print(x_cat_array.shape)\n",
    "#plt.imshow(x_cat_array[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_not_cat = []\n",
    "for filename in not_cat_files:\n",
    "    with Image.open(not_cat_dir+filename) as im:\n",
    "        if im.mode == 'RGB':\n",
    "            nim = im.resize((width, height), Image.BILINEAR)\n",
    "            pixel_values = np.array(nim.getdata()).reshape((width, height, 3))\n",
    "            x_not_cat.append(pixel_values)\n",
    "x_not_cat_array = np.array(x_not_cat)\n",
    "print(x_not_cat_array.shape)\n",
    "#plt.imshow(x_not_cat_array[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wolf = []\n",
    "for filename in wolf_files:\n",
    "    with Image.open(wolf_dir+filename) as im:\n",
    "        if im.mode == 'RGB':\n",
    "            nim = im.resize((width, height), Image.BILINEAR)\n",
    "            pixel_values = np.array(nim.getdata()).reshape((width, height, 3))\n",
    "            x_wolf.append(pixel_values)\n",
    "x_wolf_array = np.array(x_wolf)\n",
    "print(x_wolf_array.shape)\n",
    "#plt.imshow(x_wolf_array[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = []\n",
    "y_not_cat_array = np.zeros((x_not_cat_array.shape[0]+x_wolf_array.shape[0], 1))\n",
    "y_cat_array = np.ones((x_cat_array.shape[0], 1))\n",
    "\n",
    "y_array = np.append(y_not_cat_array, y_cat_array, axis=0)\n",
    "print(y_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_other_array = np.append(x_not_cat_array, x_wolf_array, axis=0)\n",
    "\n",
    "x_array = np.append(x_other_array, x_cat_array, axis=0)\n",
    "print(x_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the examples\n",
    "x_flatten = x_array.reshape(x_array.shape[0], -1).T # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1\n",
    "X = x_flatten/255.\n",
    "print(\"X's shape: \"+str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save nympy arrays into files, for later usages\n",
    "X_file = 'X.npy'\n",
    "Y_file = 'Y.npy'\n",
    "\n",
    "np.save(X_file, X)\n",
    "np.save(Y_file, y_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Read Dataset from files\n",
    "Let's read data from X.npy and Y.npy that we previously prosessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.load('X.npy')\n",
    "#Y = np.load('Y.npy').T\n",
    "#print(\"X's shape: \"+str(X.shape))\n",
    "#print(\"Y's shape: \"+str(Y.shape))\n",
    "\n",
    "# Use for verification\n",
    "x_dev = np.load('train_x.npy')\n",
    "y_dev = np.load('train_y.npy')\n",
    "x_test = np.load('test_x.npy')\n",
    "y_test = np.load('test_y.npy')\n",
    "\n",
    "#print(\"x_dev's shape: \"+str(x_dev.shape))\n",
    "#print(\"x_test's shape: \"+str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffled_X's shape: (49152, 4276)\n",
      "shuffled_Y's shape: (1, 4276)\n"
     ]
    }
   ],
   "source": [
    "# Re-shuffle X and y_array\n",
    "permutation = list(np.random.permutation(X.shape[1]))\n",
    "shuffled_X = X[:, permutation]\n",
    "shuffled_Y = Y[:, permutation]\n",
    "\n",
    "print(\"shuffled_X's shape: \"+str(shuffled_X.shape))\n",
    "print(\"shuffled_Y's shape: \"+str(shuffled_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "#index = 4001\n",
    "#plt.imshow(X[:,index].reshape((128, 128, 3)))\n",
    "#print (\"y = \" + str(Y[0,index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n"
     ]
    }
   ],
   "source": [
    "#m_test = np.rint(X.shape[1]*0.02).astype(int)\n",
    "#m_dev = m_test\n",
    "#m_train = X.shape[1]-m_test-m_dev\n",
    "#print(\"Number of training examples: \" + str(m_train))\n",
    "#print(\"Number of developing examples: \" + str(m_dev))\n",
    "#print(\"Number of testing examples: \" + str(m_test))\n",
    "\n",
    "#assert(m_test+m_dev+m_train==X.shape[1])\n",
    "\n",
    "# Use for verification\n",
    "m_train = x_dev.shape[1]\n",
    "m_test = x_test.shape[1]\n",
    "num_px = x_dev.shape[0]\n",
    "print(\"Number of training examples: \" + str(m_train))\n",
    "print(\"Number of testing examples: \" + str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data Sets|Percentage|\n",
    "|---|---|\n",
    "|Train set|96%|\n",
    "|Dev set|2%|\n",
    "|Test set|2%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (49152, 4104)\n",
      "y_train's shape: (1, 4104)\n"
     ]
    }
   ],
   "source": [
    "x_train = shuffled_X[:,0:m_train]\n",
    "y_train = shuffled_Y[:,0:m_train]\n",
    "x_dev = shuffled_X[:,m_train:m_train+m_dev]\n",
    "y_dev = shuffled_Y[:,m_train:m_train+m_dev]\n",
    "x_test = shuffled_X[:,m_train+m_dev:-1]\n",
    "y_test = shuffled_Y[:,m_train+m_dev:-1]\n",
    "\n",
    "print(\"x_train's shape: \"+str(x_train.shape))\n",
    "print(\"y_train's shape: \"+str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "#plt.imshow(x_train[:,-1].reshape((128, 128, 3)))\n",
    "#print (\"y = \" + str(y_train[0,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x_dev[:,0].reshape((128, 128, 3)))\n",
    "#print (\"y = \" + str(y_dev[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Deep Learning Model\n",
    "### 4.1 - Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ sigmoid( w^T x + b) = \\frac{1}{1 = e^{-(w^T x + b)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "        A -- output of sigmoid(z), same shape as Z\n",
    "        cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ReLU(Z) = max(0, Z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "    \n",
    "    Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        cache -- a python dictionary containing \"A\"; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initialization\n",
    "#### a) two-layer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters:\n",
    "        W1 -- weight matrix of shape (n_h, n_x)\n",
    "        b1 -- bias vector of shape (n_h, 1)\n",
    "        W2 -- weight matrix of shape (n_y, n_h)\n",
    "        b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\":W1,\n",
    "                  \"b1\":b1,\n",
    "                  \"W2\":W2,\n",
    "                  \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) L-layer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) He initialization\n",
    "$$\\text{np.random.randn(..,..)} * \\sqrt \\frac {2}{\\text{layers_dims[l-1]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)-1 # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*np.sqrt(2./layers_dims[l-1])\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward propagation module\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "where $A^{[0]} = X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        A -- activations from previous layer\n",
    "        W -- weights matrix\n",
    "        b -- bias vector\n",
    "    \n",
    "    Returns:\n",
    "        Z -- the input of the activation function\n",
    "        cache -- a python dictionary containing \"A\", \"W\" and \"b\"\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$\n",
    "- **ReLU**: $A = RELU(Z) = max(0, Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "        b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "        A -- the input of the activation function, also called the post-activation value\n",
    "        cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\"; \n",
    "                 stored for computing the backward pass effeciently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-layer activation forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- data, numpy array of shape (input size, number of examples)\n",
    "        parameters -- output of initialize_parameter_deep()\n",
    "    \n",
    "    Returns:\n",
    "        AL -- last post-activation value\n",
    "        caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Cost function\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m}(y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        AL -- probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "        Y -- true \"label\" vector, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "        cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)*np.sum(np.multiply(np.log(AL), Y)+np.multiply(np.log(1-AL),(1-Y)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Backward propagation module\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[L]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum\\limits_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "        cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "    db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient for current layer l\n",
    "        cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \n",
    "    Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-layer activation backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "        Y -- true \"label\" vector\n",
    "        caches -- list of caches cantaining:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e. l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "        grads -- A dictionary with the gradients\n",
    "            grads[\"dA\"+str(l)] = ...\n",
    "            grads[\"dW\"+str(l)] = ...\n",
    "            grads[\"db\"+str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)], grads[\"db\"+str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, \"relu\")\n",
    "        grads[\"dA\"+str(l)] = dA_prev_temp\n",
    "        grads[\"dW\"+str(l+1)] = dW_temp\n",
    "        grads[\"db\"+str(l+1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Gradient Checking\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2\\varepsilon}$$\n",
    "1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "3. $J^{+} = J(\\theta^{+})$\n",
    "4. $J^{-} = J(\\theta^{-})$\n",
    "5. $gradapprox = \\frac{J^{+} - J^{-}}{2 \\varepsilon}$\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in parameters.keys():\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta, keys, layers_dims):\n",
    "    parameters = {}\n",
    "    i = 0\n",
    "    layer_i = 1\n",
    "    #print(\"len: \"+str(len(theta)))\n",
    "    while i < len(theta):\n",
    "        index_s = keys.index(keys[i])\n",
    "        index_e = keys.count(keys[i])\n",
    "        shape_x = layers_dims[layer_i]\n",
    "        shape_y = int(index_e/shape_x)\n",
    "        #print(\"layer: \"+str(layer_i)+\", \"+str(keys[i]))\n",
    "        #print(\"index_s: \"+str(index_s))\n",
    "        #print(\"index_e: \"+str(index_e))\n",
    "        if index_s==len(keys)-1: # the last one\n",
    "            shape_x = layers_dims[-1]\n",
    "            shape_y = 1\n",
    "        else:\n",
    "            shape_x = layers_dims[layer_i]\n",
    "            shape_y = int(index_e/shape_x)\n",
    "        parameters[keys[i]] = theta[index_s:index_s+index_e].reshape((shape_x,shape_y))\n",
    "        #print(\"shape_x: \"+str(shape_x))\n",
    "        #print(\"shape_y: \"+str(shape_y))\n",
    "        i+=index_e\n",
    "        if shape_y==1:\n",
    "            layer_i+=1\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    count = 0\n",
    "    for key in gradients.keys():\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, layers_dims, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"b1\", ...\n",
    "        prad -- output of backword propagation, contains gradients of the cost with respect to the parameters\n",
    "        x -- input datapoint\n",
    "        y -- true \"label\"\n",
    "        epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "        layer_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "        \n",
    "    Returns:\n",
    "        difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters_values, keys = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    print(\"The number of parameters: %i\" %(num_parameters))\n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        thetaplus = np.copy(parameters_values)\n",
    "        thetaplus[i][0] = thetaplus[i][0]+epsilon\n",
    "        AL_plus, _= L_model_forward(X, vector_to_dictionary(thetaplus, keys, layers_dims))\n",
    "        J_plus[i] = compute_cost(AL_plus, Y)\n",
    "        \n",
    "        thetaminus = np.copy(parameters_values)\n",
    "        thetaminus[i][0] = thetaminus[i][0]-epsilon\n",
    "        AL_minus, _= L_model_forward(X, vector_to_dictionary(thetaminus, keys, layers_dims))\n",
    "        J_minus[i] = compute_cost(AL_minus, Y)\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i]-J_minus[i])/2/epsilon\n",
    "        if i % 1000 == 0:\n",
    "            print(\"gradient_check_n on %i\" %(i))\n",
    "    \n",
    "    numerator = np.linalg.norm(grad-gradapprox)\n",
    "    denominator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in the backward propagation! difference = \"+str(difference))\n",
    "    else:\n",
    "        print(\"The backward propagation works perfectly fine! difference = \"+str(difference))\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 - Update Parameters\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        parameters -- python dictionary containing parameters\n",
    "        grads -- python dictionary containing gradients, output of L_model_backward\n",
    "        \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 - Two-layer neural network\n",
    "- *LINEAR -> RELU -> LINEAR -> SIGMOID*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- input data, of shape (n_x, number of examples)\n",
    "        Y -- true \"label\" vector (containing 0 if cat, 1 if not-cat), of shape (1, number of examples)\n",
    "        layer_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_cost -- If set to True, this will print the cost every 100 iterations\n",
    "        \n",
    "    Returns:\n",
    "        parameters -- a dictionary containing W1, W2, b1 and b2\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the cost\n",
    "    m = X.shape[1] # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "        \n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # gradient checking\n",
    "        if i==num_iterations-1:\n",
    "            difference = gradient_check_n(parameters, grads, X, Y, layers_dims)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('costs')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\"+str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 - L-layer Neural Network\n",
    "- *[LINEAR -> RELU]X(L-1) -> LINEAR -> SIGMOID*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- data, numpy array of shape (number of examples, num_px*num_px*3)\n",
    "        Y -- true \"label\" vector\n",
    "        layer_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_cost -- If set to True, this will print the cost every 100 iterations\n",
    "        \n",
    "    Returns:\n",
    "        parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = [] # keep track of cost\n",
    "    #parameters = initialize_parameters_deep(layers_dims)\n",
    "    parameters = initialize_parameters_he(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # gradient checking\n",
    "        if i==num_iterations-1:\n",
    "            difference = gradient_check_n(parameters, grads, X, Y, layers_dims)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('costs')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\"+str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 - Train the model\n",
    "For developing the model, use *x_dev*. For training the model, use *x_train*.\n",
    "This step will take some time which depends on the performance of the CPU/GPU.\n",
    "\n",
    "#### a) 2-layer model\n",
    "This will take about 26mins with the dev-set on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "n_x = x_dev.shape[0] # num_px*num_px*3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693049735659989\n",
      "Cost after iteration 100: 0.6464320953428849\n",
      "Cost after iteration 200: 0.6325140647912677\n",
      "Cost after iteration 300: 0.6015024920354665\n",
      "Cost after iteration 400: 0.5601966311605747\n",
      "Cost after iteration 500: 0.5158304772764729\n",
      "Cost after iteration 600: 0.47549013139433255\n",
      "Cost after iteration 700: 0.4339163151225749\n",
      "Cost after iteration 800: 0.4007977536203885\n",
      "Cost after iteration 900: 0.3580705011323798\n",
      "Cost after iteration 1000: 0.3394281538366412\n",
      "Cost after iteration 1100: 0.30527536361962665\n",
      "Cost after iteration 1200: 0.2749137728213015\n",
      "Cost after iteration 1300: 0.2468176821061486\n",
      "Cost after iteration 1400: 0.1985073503746608\n",
      "Cost after iteration 1500: 0.17448318112556632\n",
      "Cost after iteration 1600: 0.1708076297809652\n",
      "Cost after iteration 1700: 0.11306524562164728\n",
      "Cost after iteration 1800: 0.09629426845937158\n",
      "Cost after iteration 1900: 0.0834261795972687\n",
      "Cost after iteration 2000: 0.07439078704319083\n",
      "Cost after iteration 2100: 0.06630748132267934\n",
      "Cost after iteration 2200: 0.059193295010381744\n",
      "Cost after iteration 2300: 0.053361403485605606\n",
      "Cost after iteration 2400: 0.04855478562877019\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XecVOXZ//HPtX1ZYBd2kd6LCkiRBSxojI8aOxobiBVLLKjRPM8TY/ypMY8pJnYxESxALGhswZLYYlfKIh2kiPS2tKWXhev3xxw2w7oLi+zZs7vzfb9e85qZc+45c50dmO+c+5xzH3N3REREAJKiLkBERKoPhYKIiJRQKIiISAmFgoiIlFAoiIhICYWCiIiUUChIrWBm/zSzy6OuQ6SmUyjIQTGzBWZ2UtR1uPtp7j4y6joAzOxjM7u6Ct4n3cyeMbMNZrbCzG7bT/tbg3Ybgtelx81rY2YfmdkWM/sm/jM1s7+a2aa423Yz2xg3/2Mz2xY3f3Y4ayxVQaEg1Z6ZpURdwx7VqRbgHqAj0Br4MfC/ZnZqWQ3N7CfA7cB/Be3bAb+Ja/IiMAnIBX4NvGJmjQDc/Tp3r7vnFrT9e6m3GBLX5tDKWkGpegoFCY2ZnWlmk81svZl9aWbd4ubdbmbfmtlGM5tpZufGzbvCzL4ws4fMbA1wTzDtczP7s5mtM7PvzOy0uNeU/DqvQNu2ZvZp8N4fmNlQM3uunHU4wcyWmNkvzWwF8KyZNTCzt8ysMFj+W2bWImh/H3Ac8Hjwq/nxYPphZva+ma01s9lmdmEl/IkvB37r7uvcfRYwHLhiH22fdvcZ7r4O+O2etmbWCTgSuNvdt7r7q8A04Lwy/h5ZwfRqsVUmlU+hIKEws57AM8DPiP36fBIYE9dl8S2xL89sYr9YnzOzpnGL6AvMBxoD98VNmw3kAfcDT5uZlVPCvtq+AIwP6roHuHQ/q9MEaEjsF/a1xP7fPBs8bwVsBR4HcPdfA5/xn1/OQ4Iv0veD9z0EGAA8YWady3ozM3siCNKyblODNg2ApsCUuJdOAbqUsw5dymjb2Mxyg3nz3X1jqfllLes8oBD4tNT035vZ6iDMTyinBqkBFAoSlmuBJ919nLvvCvr7twNHAbj73919mbvvdveXgLlAn7jXL3P3x9y92N23BtMWuvtwd99F7JdqU2KhUZYy25pZK6A3cJe773D3z4Ex+1mX3cR+RW8PfkmvcfdX3X1L8EV6H/Cjfbz+TGCBuz8brM8k4FXggrIau/sN7p5Tzm3P1lbd4L4o7qVFQL1yaqhbRluC9qXn7WtZlwOjfO9B035JrDuqOTAMeNPM2pdTh1RzCgUJS2vgF/G/coGWQDMAM7ssrmtpPdCV2K/6PRaXscwVex64+5bgYd0y2u2rbTNgbdy08t4rXqG7b9vzxMzqmNmTZrbQzDYQ+9WcY2bJ5by+NdC31N9iELEtkB9qU3BfP25afWBjGW33tC/dlqB96XllLisI1BOAUfHTg+DfGITmSOAL4PSKrYZUNwoFCcti4L5Sv3LruPuLZtaaWP/3ECDX3XOA6UB8V1BYw/cuBxqaWZ24aS3385rStfwCOBTo6+71geOD6VZO+8XAJ6X+FnXd/fqy3qyMo33ibzMAgv0Cy4HucS/tDswoZx1mlNF2pbuvCea1M7N6peaXXtalwBfuPr+c99jD2fuzlBpEoSCVIdXMMuJuKcS+9K8zs74Wk2VmZwRfPFnEvjgKAczsSmJbCqFz94VAAbGd12lmdjRw1gEuph6x/QjrzawhcHep+SuJdafs8RbQycwuNbPU4NbbzA4vp8a9jvYpdYvv5x8F3Bns+D4MuAYYUU7No4CrzKyzmeUAd+5p6+5zgMnA3cHndy7QjVgXV7zLSi/fzHLM7Cd7PnczG0QsJP9VTh1SzSkUpDK8Q+xLcs/tHncvIPYl9TiwDphHcLSLu88EHgC+IvYFegSxLoeqMgg4GlgD/B/wErH9HRX1MJAJrAbG8v0vwEeA84Mjkx4N9jucQmwH8zJiXVt/BNI5OHcT22G/EPgE+JO7/wtiXT3BlkUrgGD6/cBHwKLgNfFhNgDIJ/ZZ/QE4390L98wMwrMF3z8UNZXY37CQ2N/jJuCcIGikBjJdZEcSnZm9BHzj7qV/8YskHG0pSMIJum7am1mSxU726g+8EXVdItVBdTo7U6SqNAFeI3aewhLg+uAwUZGEp+4jEREpoe4jEREpUeO6j/Ly8rxNmzZRlyEiUqNMnDhxtbs32l+7GhcKbdq0oaCgIOoyRERqFDNbWJF26j4SEZESCgURESmhUBARkRKhhoKZnRpcUGSemd1exvyHgpEyJ5vZnGD0SBERiUhoO5qDYYSHAicTO0FogpmNCca9AcDdb41rfxPQM6x6RERk/8LcUugDzHP3+e6+AxhNbDiB8gwkdu1XERGJSJih0Jy9L16yJJj2PcH4+m2Bf5cz/1ozKzCzgsLCwrKaiIhIJaguO5oHAK8El078Hncf5u757p7fqNF+z70o03erN/PHf32DhvUQESlfmKGwlL2vaNUimFaWAYTcdfT+zBX85eNv+dO7s8N8GxGRGi3MM5onAB3NrC2xMBgAXFy6UXDFqAbELrgSmmuOa8d3q7fwxMff0jQnk0uPah3m24mI1EihhYK7F5vZEOBdIBl4xt1nmNm9QIG7jwmaDgBGe8j9OmbGb/t3oXDjNu7+x3Qa10vnlC4Hc910EZHap8YNnZ2fn+8HM/bRlh3FDBw+jm+Wb+CFa46iV+sGlVidiEj1ZGYT3T1/f+2qy47mKlMnLYVnLs+naXYGV4+cwPzCTVGXJCJSbSRcKADk1k1n5OA+JJlx+bPjWbVxW9QliYhUCwkZCgCtc7N45orerN64g6tGFLB5e3HUJYmIRC5hQwGge8schg7qyczlG7jh+a/ZuWt31CWJiEQqoUMB4MTDGnPfOV35ZE4hd7w2TSe3iUhCq3FXXgvDgD6tWFa0jUc/nEvTnExuO7lT1CWJiERCoRC49aSOrCjaGguG7AwG9mkVdUkiIlVOoRAwM+479whWbtjOnW9Mp3H9dE48rHHUZYmIVKmE36cQLzU5iScGHUnnpvW58flJTFmsa/6ISGJRKJSSlZ7CM1f0plG9dAaPmMC3OrlNRBKIQqEMjeqlM+LK3ux25+QHP+Hi4WN5ftxC1mzaHnVpIiKhSrixjw7E4rVbeLlgMW9NXc53qzeTnGQc0z6XM45oyk+6NKFBVlqV1CEicrAqOvaRQqEC3J1Zyzfy9rRlvDV1OQvXbCElyTimQx5ndmvKTzo3IbtOapXWJCJyIBQKIXF3ZizbwFtTl/P2tGUsXruV1GSjX4c8zujWjJM7NyY7UwEhItWLQqEKuDvTlhbx9tTlvDV1OUvXbyUtOYkbftyem07sSHKSRV2iiAigUKhy7s6UJUU8/fl3vDllGcd2yOXhi3rSqF561KWJiOh6ClXNzOjRModHB/Tg/vO7MXHhOk5/9DO+nLc66tJERCpMoVDJzIwL81vyjxv7UT8jhUFPj+PhD+awa3fN2iITkcSkUAjJoU3qMWZIP87t0ZyHP5jLZc+Mo3CjznMQkepNoRCirPQUHriwu7qTRKTGUCiETN1JIlKTKBSqiLqTRKQmCDUUzOxUM5ttZvPM7PZy2lxoZjPNbIaZvRBmPVFTd5KIVHehhYKZJQNDgdOAzsBAM+tcqk1H4FfAse7eBfh5WPVUF2V1Jz3ywVxdBlREqoUwtxT6APPcfb677wBGA/1LtbkGGOru6wDcfVWI9VQr8d1JD30wh3vGzFAwiEjkwrzyWnNgcdzzJUDfUm06AZjZF0AycI+7/6v0gszsWuBagFatas9lMvd0J+XVS2fYp/MBuOfsLphpeAwRiUbUl+NMAToCJwAtgE/N7Ah33+uSZ+4+DBgGsWEuqrrIMJkZvzrtMAAFg4hELsxQWAq0jHveIpgWbwkwzt13At+Z2RxiITEhxLqqndLB4MBvFAwiEoEwQ2EC0NHM2hILgwHAxaXavAEMBJ41szxi3UnzQ6yp2ipri0HBICJVLbRQcPdiMxsCvEtsf8Ez7j7DzO4FCtx9TDDvFDObCewC/sfd14RVU3W3JxgMeFLBICIRCHWfgru/A7xTatpdcY8duC24CbFguD3YYlAwiEhVi3pHs5ShdDC4w739FQwiEj6FQjVV1haDgkFEwqZQqMZKgsHgyU8UDCISPoVCNWdm3H5qsMXwyXwc57f9uyoYRCQUCoUaoHQwAAoGEQmFQqGGKB0MSWY6KklEKp1CoQbZEwzusRPc6mWk8D8/OSzqskSkFlEo1DB7TnDbuK2YoR99S3ZmKtce3z7qskSkllAo1EBmxv+d05UN23byu3e+ITszlYt6157RY0UkOgqFGio5yXjowh5s2lbMr16bRr2MVE4/omnUZYlIDadrNNdgaSlJ/PWSXhzZqgG3jJ7Ep3MKoy5JRGo4hUINl5mWzNNX9KbDIfX42d8mMnHh2qhLEpEaTKFQC2RnpjJqcB+aZGdw5bMTmLV8Q9QliUgNpVCoJRrVS+dvV/WhTloKlz49ngWrN0ddkojUQAqFWqRFgzo8d3UfdrtzydPjWFG0LeqSRKSGUSjUMh0OqcfIK/uwfstOLn16HOs274i6JBGpQRQKtdARLbIZflk+C9du4Ypnx7Npe3HUJYlIDaFQqKWObp/LExcfyfRlG7hmZAHbdu6KuiQRqQEUCrXYSZ0b8+cLuvHV/DXc9OIkinftjrokEanmFAq13Lk9W3Bv/y68P3Mlt7w0mZ0KBhHZBw1zkQAuO7oN23bu4nfvfEPxrt08NvBI0lL0e0BEvi/UbwYzO9XMZpvZPDO7vYz5V5hZoZlNDm5Xh1lPIrv2+PbcfVZn3p2xkuufm8j2Yu1jEJHvCy0UzCwZGAqcBnQGBppZ5zKavuTuPYLbU2HVI3DlsW35v3O68uE3q7hm1ETtfBaR7wlzS6EPMM/d57v7DmA00D/E95MKuOSo1tx/Xjc+m1vI4BET2LJDh6uKyH+EGQrNgcVxz5cE00o7z8ymmtkrZtayrAWZ2bVmVmBmBYWFGgn0YF3YuyUPXNCdsfPXcMWzE3Qeg4iUiHpv45tAG3fvBrwPjCyrkbsPc/d8d89v1KhRlRZYW/30yBY8PKAnExeu4/JnxrNh286oSxKRaiDMUFgKxP/ybxFMK+Hua9x9e/D0KaBXiPVIKWd3b8bjA3syZfF6Ln16PEVbFAwiiS7MUJgAdDSztmaWBgwAxsQ3MLP4S4WdDcwKsR4pw2lHNOUvl/Ri1rINXPzUWI2VJJLgQgsFdy8GhgDvEvuyf9ndZ5jZvWZ2dtDsZjObYWZTgJuBK8KqR8p3cufGPHlZL+au2sTA4WNZvWn7/l8kIrWSuXvUNRyQ/Px8LygoiLqMWunzuau5etQEWjaow/PX9OWQehlRlyQilcTMJrp7/v7aRb2jWaqRfh3zGHFlH5au38qAJ8fqegwiCUihIHs5ql0uowb3YdXG7Vw07CuWrd8adUkiUoUUCvI9+W0a8rer+rB20w4GDh/L8iIFg0iiUChImXq2asCoPcEwTF1JIolCoSDl6tmqASOv6sPqYItBwSBS+ykUZJ+ObNWAkYP7ULhxOwOHj2XlBgWDSG2mUJD96tW6ASMH92bVhm0MHKZgEKnNFApSIb1aN2Tk4D6s3LCNgcPHskrBIFIrKRSkwvLbNGTE4D6sKAqCYaOCQaS2USjIAendJrbFsLwo1pVUuFFDYojUJgoFOWC92zRkxJVBMAxXMIjUJgoF+UH6tG3Is1f0Zum6rVysYBCpNRQK8oP1bZfLs1f2Zsm6rQx6SqOritQGCgU5KEe1y+WZK3qzaO0WBg0fxxoFg0iNplCQg3Z0+1gwLFy7mQue/IpvCzdFXZKI/EAKBakUx7TPY9TgvqzfspNzHv+CD2etjLokEfkBFApSafq0bciYIcfSOq8OV48q4NEP57J7d826iJNIolMoSKVq0aAOr1x3DOf0aM6D78/huucmsml7cdRliUgFKRSk0mWkJvPghd2568zOfPjNKs4Z+gXztZ9BpEZQKEgozIzB/drGLtazeQf9h37Bv7/RfgaR6k6hIKE6pn0eY4YcS6uGdbhqZAGPaT+DSLWmUJDQ7dnP0L97Mx54fw7XP6/9DCLVVaihYGanmtlsM5tnZrfvo915ZuZmlh9mPRKdzLRkHrqoB3eecTgfzFrFuUO/4LvVm6MuS0RKCS0UzCwZGAqcBnQGBppZ5zLa1QNuAcaFVYtUD2bG1ce142+D+7B603bOfvxzPvpmVdRliUicCoWCmWWZWVLwuJOZnW1mqft5WR9gnrvPd/cdwGigfxntfgv8EdDg/AnimA55jBnSj5YN6jB45ARGfbUg6pJEJFDRLYVPgQwzaw68B1wKjNjPa5oDi+OeLwmmlTCzI4GW7v72vhZkZteaWYGZFRQWFlawZKnOWjasw6vXH8NJhzfmrn/M4MXxi6IuSUSoeCiYu28Bfgo84e4XAF0O5o2DLY8HgV/sr627D3P3fHfPb9So0cG8rVQjmWnJPH5xT358aCPueH0ar05cEnVJIgmvwqFgZkcDg4A9v+qT9/OapUDLuOctgml71AO6Ah+b2QLgKGCMdjYnlvSUZP5ySS+OaZ/L/7wyhTenLIu6JJGEVtFQuAX4FfC6u88ws3bAR/t5zQSgo5m1NbM0YAAwZs9Mdy9y9zx3b+PubYCxwNnuXnDAayE1WkZqMsMvyye/dUN+/tJk3p2xIuqSRBJWRUOhsbuf7e5/BHD3+cBn+3qBuxcDQ4B3gVnAy0Gg3GtmZx9M0VL71ElL4Zkre9OtRTZDXvhaRyWJRMTc9392qZl97e5H7m9aVcjPz/eCAm1M1FZFW3cy6KmxzFm5iWev6M2xHfKiLkmkVjCzie6+3+75fW4pmNlpZvYY0NzMHo27jQB0SqpUuuzMVP42uC/t8rK4auQExs1fE3VJIgllf91Hy4ACYucQTIy7jQF+Em5pkqgaZKXxt6v60jwnk8EjJvD1onVRlySSMPYZCu4+xd1HAh3cfWTweAyxk9L0P1VC06heOi9ccxR59dK5/JnxTF9aFHVJIgmhojua3zez+mbWEPgaGG5mD4VYlwiN62fwwjVHUT8jlUueHsc3KzZEXZJIrVfRUMh29w3ETl4b5e59gf8KryyRmOY5mbxwTV8yUpIZNHwc81ZtjLokkVqtoqGQYmZNgQuBt0KsR+R7Wudm8fw1fTEzLh4+jgUaXVUkNBUNhXuJnW/wrbtPCE5emxteWSJ7a9+oLi9c05fi3c7A4WN5Y9JSdu7aHXVZIrVOhc5TqE50nkJim7GsiFtGT2beqk00zc7gimPaMLBvK+pn7G/QXpHEVinnKcQtrIWZvW5mq4Lbq2bW4uDLFDkwXZpl897Pj+fZK3rTJjeL3//zG475/b/57VszWbJuS9TlidR4FT2j+X3gBeBvwaRLgEHufnKItZVJWwoSb/rSIp76bD5vTl0OwOlHNOWa49rSrUVOxJWJVC8V3VKoaChMdvce+5tWFRQKUpZl67cy4ssFvDhuERu3F9O3bUOuOa4dJx52CElJFnV5IpGr1O4jYI2ZXWJmycHtEkDjD0i10SwnkztOP5wvf3Uid55xOIvXbuHqUQWc9NAnvDh+Edt27oq6RJEaoaJbCq2Bx4CjAQe+BG5y98X7fGEItKUgFbFz127embac4Z/NZ/rSDbRsmMnwy/I5rEn9qEsTiURlbyncC1zu7o3c/RBgMPCbgylQJEypyUn079GcN4f047mr+rJ9527Oe+JL3tO1GkT2qaKh0C1+rCN3Xwv0DKckkcpjZvTrmMebN/WjwyF1+dlzExn60Txq2qHYIlWloqGQZGYN9jwJxkBKCackkcrXuH4GL/3saM7q1ow/vTubn780WfsZRMpQ0S/2B4CvzOzvwfMLgPvCKUkkHBmpyTwyoAeHNqnHn96dzYLVmxl2WT6N62dEXZpItVGhLQV3H0VsMLyVwe2n7v63fb9KpPoxM278cQeGXdqLuas2cfbjnzN1yfqoyxKpNirafYS7z3T3x4PbzDCLEgnbKV2a8Or1x5CSlMQFf/2KMVOWRV2SSLVQ4VAQqW0Ob1qfMUOOpXuLHG5+cRJ/fnc2u3drB7QkNoWCJLTcuuk8d3VfBvRuyeMfzeO65yayebsuPy6JS6EgCS8tJYnf//QI7j6rMx/MWsl5f/mSxWs1uJ4kplBDwcxONbPZZjbPzG4vY/51ZjbNzCab2edm1jnMekTKY2ZceWxbRlzZh6Xrt9J/6Bd89a1GcpHEE1oomFkyMBQ4DegMDCzjS/8Fdz8iGFjvfuDBsOoRqYjjOzXiHzceS06dVAY9NZaH3p/DLu1nkAQS5pZCH2Ceu8939x3AaKB/fIPgus97ZBEbV0kkUu0a1eXNIf04t2cLHvlwLgOHj2V50daoyxKpEmGGQnMgfsC8JcG0vZjZjWb2LbEthZvLWpCZXWtmBWZWUFhYGEqxIvGy0lN44MLuPHhhd6YvLeK0Rz7jg5kroy5LJHSR72h296Hu3h74JXBnOW2GuXu+u+c3atSoaguUhPbTI1vw1k39aJ6TydWjCvjNmzPYXqzhMaT2CjMUlgIt4563CKaVZzRwToj1iPwg7RrV5bUbjuHKY9vw7BcL+OkTX/Ld6s1RlyUSijBDYQLQ0czamlkaMAAYE9/AzDrGPT0DmBtiPSI/WHpKMnef1YXhl+WzdP1Wznz0M16ftCTqskQqXWih4O7FwBDgXWAW8LK7zzCze83s7KDZEDObYWaTgduAy8OqR6QynNy5Mf+85Ti6NMvm1pem8IuXp+hkN6lVKnTltepEV16T6qB4124e+/c8Hvv3XNrkZvHYxT3p0iw76rJEylXZV14TkTgpyUncenInnr/6KDbvKObcoV8y8ssFuniP1HgKBZGDcHT7XP55y/H065jH3WNmcPmzE1hRtC3qskR+MIWCyEFqmJXG05fn89v+XZjw3VpOeegTXp+0RFsNUiMpFEQqgZlx6dFt+Octx3Fok3rc+tIUrntuIqs3bY+6NJEDolAQqURt8rIYfe3R/Pr0w/lodiGnPPQp/5q+POqyRCpMoSBSyZKTjGuOb8fbwZnQ1z33NT8fPYmiLTujLk1kvxQKIiHp2Lger91wDLee1Im3pi7nlIc/4aPZq6IuS2SfFAoiIUpNTuKWkzryxo3Hkp2ZypXPTuD2V6eycZu2GqR6UiiIVIGuzbN586Z+XH9Ce14uWMypD3/Gl9+ujroske9RKIhUkfSUZH556mH8/bpjSEtJ4uLh4/j9O7N06KpUKwoFkSrWq3UD3rn5OAb2acWTn85n6Efzoi5JpERK1AWIJKLMtGR+d25Xtu4o5s/vzaF1bhZndW8WdVki2lIQiYqZ8cfzu9G7TQN+8fcpTFy4NuqSRBQKIlFKT0lm2KX5NMvO4JpRE1m0ZkvUJUmCUyiIRKxBVhrPXNGb3e5cOWK8TnKTSCkURKqBdo3q8uQlvVi0dgvXPz+RHcW7oy5JEpRCQaSa6Nsulz/8tBtffruGO9+YpkNVJRI6+kikGjmvVwsWrtnMo/+eR5u8LG44oUPUJUmCUSiIVDO3ntyJBWu2cP+/ZtO6YRZndGsadUmSQNR9JFLNmBn3n9+N/NYNuO3lyXy9aF3UJUkCUSiIVEMZqck8eWkvGtfP4JqRBSxeq0NVpWqEGgpmdqqZzTazeWZ2exnzbzOzmWY21cw+NLPWYdYjUpPk1k3nmSt6s3PXbq4cMYGirTpUVcIXWiiYWTIwFDgN6AwMNLPOpZpNAvLdvRvwCnB/WPWI1EQdDqnLk5fms3DNZm54fiI7d+lQVQlXmFsKfYB57j7f3XcAo4H+8Q3c/SN337NdPBZoEWI9IjXS0e1z+f1Pu/HFvDX8vzem61BVCVWYRx81BxbHPV8C9N1H+6uAf5Y1w8yuBa4FaNWqVWXVJ1JjnN+rBQtWb+bxj+aRnGT876mHkZ2ZGnVZUgtVi0NSzewSIB/4UVnz3X0YMAwgPz9fP5MkId12cie27NjFs19+xz+nr+C/TzmUi3q3JDnJoi5NapEwu4+WAi3jnrcIpu3FzE4Cfg2c7e7bQ6xHpEZLSjLuOqszbw7pR4dGdbnj9Wmc/fjnjP9Oo6tK5QkzFCYAHc2srZmlAQOAMfENzKwn8CSxQNAVzUUqoGvzbF762VE8NrAn6zbv4MInv+KmFyexbP3WqEuTWiC0UHD3YmAI8C4wC3jZ3WeY2b1mdnbQ7E9AXeDvZjbZzMaUszgRiWNmnNW9GR/+4gRu/q+OvDdjBSc+8DGPfDCXbTt3RV2e1GBW045kyM/P94KCgqjLEKlWlqzbwu/f+Ya3py2neU4md5x+OKcf0QQz7W+QGDOb6O75+2unM5pFaoEWDeowdNCRjL72KOpnpnLjC18zcPhYZi3fEHVpUsNoS0Gkltm123lx/CIeeG82RVt3clHvlhzZqgF59dLJy0onr14auVnppKXoN2EiqeiWgkJBpJYq2rKThz6Yw3NjF1K8+/v/z7MzU8mtm0Ze3XQa1U0veZxXN53uLbPp0iw7gqolLAoFEQFg645dFG7cTuGm7azZtJ3Vm3awetN2Vm/azppNOygMHq/euJ0N24oBSEkyXrvhGLq1yIm4eqksFQ2FanHymoiEJzMtmVa5dWiVW2e/bXcU72bZ+q1cPHwst4yezFs39SMrXV8TiUSdiiJSIi0liTZ5WTx4UQ8WrNnMb96cEXVJUsUUCiLyPUe1y+WGE9rzcsES3p66POpypAopFESkTD8/qRPdW+bwq9emslRnSycMhYKIlCk1OYlHB/Rg127n1pcms6uMI5ik9lEoiEi5Wudm8Zv+XRn/3Vr++sm3UZcjVUChICL7dN6RzTmrezMefH8Okxati7ocCZlCQUT2ycz4v3O60qR+BreMnsym7cVRlyQhUiiIyH5lZ6by8IAeLFm3hbv/ocNUazOFgohUSO82DRlyYkde/XoJY6Ysi7ocCYlCQUQq7OYTO3Bkqxx+/fo0lqzbEnU5EgKFgohUWEpyEo8M6Ik7/Hz0ZIp37Y66JKnqmAlnAAANBklEQVRkCgUROSAtG9bht+d0oWDhOoZ+pMNUaxuFgogcsHN7tuCcHs149N9zmbhQh6nWJgoFEflB7j2nK02zM7hl9CQ2bNsZdTlSSRQKIvKD1M9I5ZEBPVhetI273pgedTlSSRQKIvKD9WrdkJtP7Mgbk5fx0PtzKNqiLYaaTlfPEJGDcuOP2zNtaRGPfDiXv37yLWd1b8agvq3o0TIHM4u6PDlAoW4pmNmpZjbbzOaZ2e1lzD/ezL42s2IzOz/MWkQkHCnJSTx1eT5v3dSP83q14J/TlnPuE19yxqOf89zYhRoWo4YJ7RrNZpYMzAFOBpYAE4CB7j4zrk0boD7w38AYd39lf8vVNZpFqrdN24v5x+SlPDd2EbOWbyArLZn+PZszqG8rujTLjrq8hFUdrtHcB5jn7vODgkYD/YGSUHD3BcE8nQEjUkvUTU9hUN/WXNynFZMXr+f5cYt4deISXhi3iB4tcxjUtxVndmtGZlpy1KVKGcLsPmoOLI57viSYdsDM7FozKzCzgsLCwkopTkTCZWb0bNWAP1/QnfF3nMRdZ3Zm47ad/M8rU+n7uw+4Z8wMJi5cx25dvKdaqRE7mt19GDAMYt1HEZcjIgcou04qg/u15cpj2zD+u7U8P24Rz49byIgvF5BXN52TOx/CKZ2bcHT7XDJStQURpTBDYSnQMu55i2CaiCQoM6Nvu1z6tsulaGtXPp69ivdmrmTM5GW8OH4xWWnJ/OjQRpzSuQk/PvQQsuukRl1ywgkzFCYAHc2sLbEwGABcHOL7iUgNkp2ZSv8ezenfoznbi3fx1bdreG/mSt6fuZJ3pq0gJcno264hp3RuwsmdG9MsJzPqkhNCaEcfAZjZ6cDDQDLwjLvfZ2b3AgXuPsbMegOvAw2AbcAKd++yr2Xq6COR2m33bmfKkvW8N3Ml781YwbeFmwHo2rw+Jx7WmPzWDejeMofsTG1FHIiKHn0UaiiEQaEgkli+LdzE+0FATFq8nj1fWR0OqUvPljn0aJVDz5YN6NS4LinJGqShPAoFEal1Nm7bybQlRUxavJ5Ji9bx9aL1rN28A4A6acl0a5FNz1YN6Nkyh56tGtCoXnrEFVcf1eE8BRGRSlUvI5VjOuRxTIc8ANydxWu3MmnxOiYtigXF8E/nUxwc5tqiQSbdW+TQpXl9jmieTZdm2TTMSotyFao9hYKI1FhmRqvcOrTKrUP/HrHToLbt3MWMZUVMWrSerxetY+rS9bw9bXnJa5rnZNKlWX26Ns+ma/P6dG2WzSH1M6JahWpHoSAitUpGajK9WjekV+uGJdOKtuxkxrIipi8rYvrSDUxfVsT7s1aW7J9oVC+drkFQdGmWzeFN69GyQR2SkhJvQD+FgojUetl19u52gtgYTbOWb2DaklhYzFi6gU/mFLLnBOvM1GQ6Nq5Lp8b1OLRxPTo1id03rp9eq0d/VSiISEKqm55C7zYN6d3mP1sUW3fs4psVG5izciOzV2xizsqNfDKnkFcmLilpUz8jhUOb1IuFRXDfqXE9GtRJrRVhoVAQEQlkpiXHjl5q1WCv6Ws372DOyo1BWMTu35yyjOfH/WdY8PoZKbTJy6J1bhZtcuvsdZ9XN63GBIZCQURkPxpmpXFUu1yOapdbMs3dWblhO7NXbmTuyo0sWruF71ZvZsri9bw9dRnx4/zVTU+hdW4d2uRmldy3aJhJ85xMmmRnkJ5SfcZ7UiiIiPwAZkaT7AyaZGfwo06N9pq3o3g3S9dvZcGazSxcvZkFa7awYM1mZi7fwLszVpQcMrtHXt10mudk0Cwnk2Y5mTTNzqB58LhZTia5WWlVttNboSAiUsnSUpJom5dF27wsOHTvecW7YoGxdN1Wlq7fyvKibSxbH3s8Z+VGPp5dyNadu/ZeXnISTXMyuO3kTiWH3oZFoSAiUoVSkpNonRvb91AWd6do685YYKzfxrKirSWPc7PCP0NboSAiUo2YGTl10sipkxbJ5Us1epSIiJRQKIiISAmFgoiIlFAoiIhICYWCiIiUUCiIiEgJhYKIiJRQKIiISIkad41mMysEFv7Al+cBqyuxnJomkdc/kdcdEnv9te4xrd290b4aQw0MhYNhZgUVuXB1bZXI65/I6w6Jvf5a9wNbd3UfiYhICYWCiIiUSLRQGBZ1ARFL5PVP5HWHxF5/rfsBSKh9CiIism+JtqUgIiL7oFAQEZESCRMKZnaqmc02s3lmdnvU9VQlM1tgZtPMbLKZFURdT9jM7BkzW2Vm0+OmNTSz981sbnDfIMoaw1LOut9jZkuDz3+ymZ0eZY1hMbOWZvaRmc00sxlmdkswPVE++/LW/4A+/4TYp2BmycAc4GRgCTABGOjuMyMtrIqY2QIg390T4gQeMzse2ASMcveuwbT7gbXu/ofgR0EDd/9llHWGoZx1vwfY5O5/jrK2sJlZU6Cpu39tZvWAicA5wBUkxmdf3vpfyAF8/omypdAHmOfu8919BzAa6B9xTRISd/8UWFtqcn9gZPB4JLH/LLVOOeueENx9ubt/HTzeCMwCmpM4n315639AEiUUmgOL454v4Qf8sWowB94zs4lmdm3UxUSksbsvDx6vABpHWUwEhpjZ1KB7qVZ2n8QzszZAT2AcCfjZl1p/OIDPP1FCIdH1c/cjgdOAG4MuhoTlsT7T2t9v+h9/AdoDPYDlwAPRlhMuM6sLvAr83N03xM9LhM++jPU/oM8/UUJhKdAy7nmLYFpCcPelwf0q4HVi3WmJZmXQ57qn73VVxPVUGXdf6e673H03MJxa/PmbWSqxL8Tn3f21YHLCfPZlrf+Bfv6JEgoTgI5m1tbM0oABwJiIa6oSZpYV7HTCzLKAU4Dp+35VrTQGuDx4fDnwjwhrqVJ7vhAD51JLP38zM+BpYJa7Pxg3KyE++/LW/0A//4Q4+gggOAzrYSAZeMbd74u4pCphZu2IbR0ApAAv1PZ1N7MXgROIDRu8ErgbeAN4GWhFbOj1C9291u2QLWfdTyDWdeDAAuBncX3stYaZ9QM+A6YBu4PJdxDrV0+Ez7689R/IAXz+CRMKIiKyf4nSfSQiIhWgUBARkRIKBRERKaFQEBGREgoFEREpoVCQasPMvgzu25jZxZW87DvKeq+wmNk5ZnZXSMu+Y/+tDniZR5jZiMpertQ8OiRVqh0zOwH4b3c/8wBek+LuxfuYv8nd61ZGfRWs50vg7IMdmbas9QprXczsA2Cwuy+q7GVLzaEtBak2zGxT8PAPwHHB2O+3mlmymf3JzCYEg3r9LGh/gpl9ZmZjgJnBtDeCgf9m7Bn8z8z+AGQGy3s+/r0s5k9mNt1i15y4KG7ZH5vZK2b2jZk9H5wxipn9IRizfqqZfW84YjPrBGzfEwhmNsLM/mpmBWY2x8zODKZXeL3ill3WulxiZuODaU8GQ8VjZpvM7D4zm2JmY82scTD9gmB9p5jZp3GLf5PY2f6SyNxdN92qxY3YmO8QOwP3rbjp1wJ3Bo/TgQKgbdBuM9A2rm3D4D6T2On8ufHLLuO9zgPeJ3ame2NgEdA0WHYRsXGykoCvgH5ALjCb/2xl55SxHlcCD8Q9HwH8K1hOR2Kj9GYcyHqVVXvw+HBiX+apwfMngMuCxw6cFTy+P+69pgHNS9cPHAu8GfW/A92ivaVUNDxEInQK0M3Mzg+eZxP7ct0BjHf37+La3mxm5waPWwbt1uxj2f2AF919F7GB0z4BegMbgmUvATCzyUAbYCywDXjazN4C3ipjmU2BwlLTXvbYgGRzzWw+cNgBrld5/gvoBUwINmQy+c+Abzvi6ptI7CJTAF8AI8zsZeC1/yyKVUCzCryn1GIKBakJDLjJ3d/da2Js38PmUs9PAo529y1m9jGxX+Q/1Pa4x7uAFHcvNrM+xL6MzweGACeWet1WYl/w8UrvvHMquF77YcBId/9VGfN2uvue991F8P/d3a8zs77AGcBEM+vl7muI/a22VvB9pZbSPgWpjjYC9eKevwtcHwwLjJl1CkZ8LS0bWBcEwmHAUXHzdu55fSmfARcF/fuNgOOB8eUVZrGx6rPd/R3gVqB7Gc1mAR1KTbvAzJLMrD3QjlgXVEXXq7T4dfkQON/MDgmW0dDMWu/rxWbW3t3HuftdxLZo9gwr34laOoKqVJy2FKQ6mgrsMrMpxPrjHyHWdfN1sLO3kLIvqfgv4Dozm0XsS3ds3LxhwFQz+9rdB8VNfx04GphC7Nf7/7r7iiBUylIP+IeZZRD7lX5bGW0+BR4wM4v7pb6IWNjUB65z921m9lQF16u0vdbFzO4kdmW9JGAncCOx0UDL8ycz6xjU/2Gw7gA/Bt6uwPtLLaZDUkVCYGaPENtp+0Fw/P9b7v5KxGWVy8zSgU+IXaWv3EN7pfZT95FIOH4H1Im6iAPQCrhdgSDaUhARkRLaUhARkRIKBRERKaFQEBGREgoFEREpoVAQEZES/x+l/NbspIKz2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105929ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 47.91081690788269 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = tm.time()\n",
    "parameters_2L = two_layer_model(x_dev, y_dev, layers_dims=(n_x, n_h, n_y), num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 0.693049753... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.04855478... </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) L-layer model\n",
    "This will take about 33mins with the dev-set on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "layers_dims = [x_dev.shape[0], 20, 7, 5, 1] # 4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "gradient_check_n on 0\n",
      "gradient_check_n on 1000\n",
      "gradient_check_n on 2000\n",
      "gradient_check_n on 3000\n",
      "gradient_check_n on 4000\n",
      "gradient_check_n on 5000\n",
      "gradient_check_n on 6000\n",
      "gradient_check_n on 7000\n",
      "gradient_check_n on 8000\n",
      "gradient_check_n on 9000\n",
      "gradient_check_n on 10000\n",
      "gradient_check_n on 11000\n",
      "gradient_check_n on 12000\n",
      "gradient_check_n on 13000\n",
      "gradient_check_n on 14000\n",
      "gradient_check_n on 15000\n",
      "gradient_check_n on 16000\n",
      "gradient_check_n on 17000\n",
      "gradient_check_n on 18000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-f8e5cae12e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparameters_NL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-33a278e4b78e>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# gradient checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_check_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-4bb9d646c147>\u001b[0m in \u001b[0;36mgradient_check_n\u001b[0;34m(parameters, gradients, X, Y, layers_dims, epsilon)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mthetaminus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mthetaminus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthetaminus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mAL_minus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_to_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetaminus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mJ_minus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL_minus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-0680bbcc0209>\u001b[0m in \u001b[0;36mvector_to_dictionary\u001b[0;34m(theta, keys, layers_dims)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mindex_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mindex_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mshape_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mshape_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_e\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mshape_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = tm.time()\n",
    "parameters_NL = L_layer_model(x_dev, y_dev, layers_dims, num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan=\"2\">Random Initialization</th><th colspan=\"2\">He Initialization</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 0.771749 </td>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 1.208125 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.092878 </td>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.505814 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 -- Calculate the accurancy of the predicting results\n",
    "Predict the results and the accuracy rate.\n",
    "$$\\hat{Y} = A = \\sigma(w^T X + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- data set of examples\n",
    "        parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "        p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    \n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "            \n",
    "    print(\"Accuracy: \"+str(np.sum((p==y)/m)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8233918128654969\n"
     ]
    }
   ],
   "source": [
    "pred_2L_dev = predict(x_dev, y_dev, parameters_2L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 1.0 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9087719298245611\n"
     ]
    }
   ],
   "source": [
    "pred_NL_dev = predict(x_dev, y_dev, parameters_NL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Train Accuracy**</td>\n",
    "        <td> 0.985645933014 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6557377049180327\n"
     ]
    }
   ],
   "source": [
    "pred_2L_test_d = predict(x_test, y_test, parameters_2L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 0.72 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.563231850117096\n"
     ]
    }
   ],
   "source": [
    "pred_NL_test_d = predict(x_test, y_test, parameters_NL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 0.8 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Train the model with training data\n",
    "### 5.1 2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "n_x = x_train.shape[0] # 128*128*3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = tm.time()\n",
    "parameters_2L_t = two_layer_model(x_train, y_train, layers_dims=(n_x, n_h, n_y), num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 L-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "layers_dims = [x_train.shape[0], 20, 7, 5, 1] # 4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = tm.time()\n",
    "parameters_NL_t = L_layer_model(x_train, y_train, layers_dims, num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Accuracy rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2L_train = predict(x_train, y_train, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NL_train = predict(x_train, y_train, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Verify with testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2L_test = predict(x_test, y_test, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NL_test = predict(x_train, y_train, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
