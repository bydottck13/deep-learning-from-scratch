{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning from scratch\n",
    "## Deep Neural Networks for a cat classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import time as tm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Pre-processing images\n",
    "Skip this step after the first time running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dir = 'cat/'\n",
    "not_cat_dir = 'not-cat/'\n",
    "wolf_dir = 'wolf/'\n",
    "\n",
    "cat_files = [f for f in listdir(cat_dir) if isfile(join(cat_dir, f))]\n",
    "not_cat_files = [f for f in listdir(not_cat_dir) if isfile(join(not_cat_dir, f))]\n",
    "wolf_files = [f for f in listdir(wolf_dir) if isfile(join(wolf_dir, f))]\n",
    "\n",
    "width = 128\n",
    "height = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = []\n",
    "for filename in cat_files:\n",
    "    with Image.open(cat_dir+filename) as im:\n",
    "        if im.mode == 'RGB':\n",
    "            nim = im.resize((width, height), Image.BILINEAR)\n",
    "            pixel_values = np.array(nim.getdata()).reshape((width, height, 3))\n",
    "            x_cat.append(pixel_values)\n",
    "x_cat_array = np.array(x_cat)\n",
    "print(x_cat_array.shape)\n",
    "#plt.imshow(x_cat_array[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_not_cat = []\n",
    "for filename in not_cat_files:\n",
    "    with Image.open(not_cat_dir+filename) as im:\n",
    "        if im.mode == 'RGB':\n",
    "            nim = im.resize((width, height), Image.BILINEAR)\n",
    "            pixel_values = np.array(nim.getdata()).reshape((width, height, 3))\n",
    "            x_not_cat.append(pixel_values)\n",
    "x_not_cat_array = np.array(x_not_cat)\n",
    "print(x_not_cat_array.shape)\n",
    "#plt.imshow(x_not_cat_array[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wolf = []\n",
    "for filename in wolf_files:\n",
    "    with Image.open(wolf_dir+filename) as im:\n",
    "        if im.mode == 'RGB':\n",
    "            nim = im.resize((width, height), Image.BILINEAR)\n",
    "            pixel_values = np.array(nim.getdata()).reshape((width, height, 3))\n",
    "            x_wolf.append(pixel_values)\n",
    "x_wolf_array = np.array(x_wolf)\n",
    "print(x_wolf_array.shape)\n",
    "#plt.imshow(x_wolf_array[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = []\n",
    "y_not_cat_array = np.zeros((x_not_cat_array.shape[0]+x_wolf_array.shape[0], 1))\n",
    "y_cat_array = np.ones((x_cat_array.shape[0], 1))\n",
    "\n",
    "y_array = np.append(y_not_cat_array, y_cat_array, axis=0)\n",
    "print(y_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_other_array = np.append(x_not_cat_array, x_wolf_array, axis=0)\n",
    "\n",
    "x_array = np.append(x_other_array, x_cat_array, axis=0)\n",
    "print(x_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the examples\n",
    "x_flatten = x_array.reshape(x_array.shape[0], -1).T # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1\n",
    "X = x_flatten/255.\n",
    "print(\"X's shape: \"+str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save nympy arrays into files, for later usages\n",
    "X_file = 'X.npy'\n",
    "Y_file = 'Y.npy'\n",
    "\n",
    "np.save(X_file, X)\n",
    "np.save(Y_file, y_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Read Dataset from files\n",
    "Let's read data from X.npy and Y.npy that we previously prosessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape: (49152, 4276)\n",
      "Y's shape: (1, 4276)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('X.npy')\n",
    "Y = np.load('Y.npy').T\n",
    "print(\"X's shape: \"+str(X.shape))\n",
    "print(\"Y's shape: \"+str(Y.shape))\n",
    "\n",
    "# Use for verification\n",
    "#x_dev = np.load('train_x.npy')\n",
    "#y_dev = np.load('train_y.npy')\n",
    "#x_test = np.load('test_x.npy')\n",
    "#y_test = np.load('test_y.npy')\n",
    "\n",
    "#print(\"x_dev's shape: \"+str(x_dev.shape))\n",
    "#print(\"x_test's shape: \"+str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffled_X's shape: (49152, 4276)\n",
      "shuffled_Y's shape: (1, 4276)\n"
     ]
    }
   ],
   "source": [
    "# Re-shuffle X and y_array\n",
    "permutation = list(np.random.permutation(X.shape[1]))\n",
    "shuffled_X = X[:, permutation]\n",
    "shuffled_Y = Y[:, permutation]\n",
    "\n",
    "print(\"shuffled_X's shape: \"+str(shuffled_X.shape))\n",
    "print(\"shuffled_Y's shape: \"+str(shuffled_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "#index = 4001\n",
    "#plt.imshow(X[:,index].reshape((128, 128, 3)))\n",
    "#print (\"y = \" + str(Y[0,index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2566\n",
      "Number of developing examples: 855\n",
      "Number of testing examples: 855\n"
     ]
    }
   ],
   "source": [
    "m_test = np.rint(X.shape[1]*0.02).astype(int)\n",
    "m_dev = m_test\n",
    "m_train = X.shape[1]-m_test-m_dev\n",
    "print(\"Number of training examples: \" + str(m_train))\n",
    "print(\"Number of developing examples: \" + str(m_dev))\n",
    "print(\"Number of testing examples: \" + str(m_test))\n",
    "\n",
    "assert(m_test+m_dev+m_train==X.shape[1])\n",
    "\n",
    "# Use for verification\n",
    "#m_train = x_dev.shape[1]\n",
    "#m_test = x_test.shape[1]\n",
    "#num_px = x_dev.shape[0]\n",
    "#print(\"Number of training examples: \" + str(m_train))\n",
    "#print(\"Number of testing examples: \" + str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data Sets|Percentage|\n",
    "|---|---|\n",
    "|Train set|80%|\n",
    "|Dev set|20%|\n",
    "|Test set|20%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (49152, 2566)\n",
      "y_train's shape: (1, 2566)\n"
     ]
    }
   ],
   "source": [
    "x_train = shuffled_X[:,0:m_train]\n",
    "y_train = shuffled_Y[:,0:m_train]\n",
    "x_dev = shuffled_X[:,m_train:m_train+m_dev]\n",
    "y_dev = shuffled_Y[:,m_train:m_train+m_dev]\n",
    "x_test = shuffled_X[:,m_train+m_dev:-1]\n",
    "y_test = shuffled_Y[:,m_train+m_dev:-1]\n",
    "\n",
    "print(\"x_train's shape: \"+str(x_train.shape))\n",
    "print(\"y_train's shape: \"+str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "#plt.imshow(x_train[:,-1].reshape((128, 128, 3)))\n",
    "#print (\"y = \" + str(y_train[0,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x_dev[:,0].reshape((128, 128, 3)))\n",
    "#print (\"y = \" + str(y_dev[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Deep Learning Model\n",
    "### 4.1 - Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ sigmoid( w^T x + b) = \\frac{1}{1 = e^{-(w^T x + b)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "        A -- output of sigmoid(z), same shape as Z\n",
    "        cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ReLU(Z) = max(0, Z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        Z -- Output of the linear layer, of any shape\n",
    "    \n",
    "    Returns:\n",
    "        A -- Post-activation parameter, of the same shape as Z\n",
    "        cache -- a python dictionary containing \"A\"; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initialization\n",
    "#### a) two-layer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters:\n",
    "        W1 -- weight matrix of shape (n_h, n_x)\n",
    "        b1 -- bias vector of shape (n_h, 1)\n",
    "        W2 -- weight matrix of shape (n_y, n_h)\n",
    "        b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\":W1,\n",
    "                  \"b1\":b1,\n",
    "                  \"W2\":W2,\n",
    "                  \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) L-layer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) He initialization\n",
    "$$\\text{np.random.randn(..,..)} * \\sqrt \\frac {2}{\\text{layers_dims[l-1]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)-1 # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*np.sqrt(2./layers_dims[l-1])\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward propagation module\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "where $A^{[0]} = X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        A -- activations from previous layer\n",
    "        W -- weights matrix\n",
    "        b -- bias vector\n",
    "    \n",
    "    Returns:\n",
    "        Z -- the input of the activation function\n",
    "        cache -- a python dictionary containing \"A\", \"W\" and \"b\"\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$\n",
    "- **ReLU**: $A = RELU(Z) = max(0, Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "        b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "        A -- the input of the activation function, also called the post-activation value\n",
    "        cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\"; \n",
    "                 stored for computing the backward pass effeciently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-layer activation forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- data, numpy array of shape (input size, number of examples)\n",
    "        parameters -- output of initialize_parameter_deep()\n",
    "    \n",
    "    Returns:\n",
    "        AL -- last post-activation value\n",
    "        caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Cost function\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m}(y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        AL -- probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "        Y -- true \"label\" vector, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "        cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)*np.sum(np.multiply(np.log(AL), Y)+np.multiply(np.log(1-AL),(1-Y)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Backward propagation module\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[L]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum\\limits_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "        cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "    db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient for current layer l\n",
    "        cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "        \n",
    "    Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "        dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "        db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-layer activation backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "        Y -- true \"label\" vector\n",
    "        caches -- list of caches cantaining:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e. l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "        grads -- A dictionary with the gradients\n",
    "            grads[\"dA\"+str(l)] = ...\n",
    "            grads[\"dW\"+str(l)] = ...\n",
    "            grads[\"db\"+str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)], grads[\"db\"+str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, \"relu\")\n",
    "        grads[\"dA\"+str(l)] = dA_prev_temp\n",
    "        grads[\"dW\"+str(l+1)] = dW_temp\n",
    "        grads[\"db\"+str(l+1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Gradient Checking\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2\\varepsilon}$$\n",
    "1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "3. $J^{+} = J(\\theta^{+})$\n",
    "4. $J^{-} = J(\\theta^{-})$\n",
    "5. $gradapprox = \\frac{J^{+} - J^{-}}{2 \\varepsilon}$\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in parameters.keys():\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    print(str(keys[0])+\" \"+str(keys[0]))\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    parameters = {}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    count = 0\n",
    "    for key in gradients.keys():\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"b1\", ...\n",
    "        prad -- output of backword propagation, contains gradients of the cost with respect to the parameters\n",
    "        x -- input datapoint\n",
    "        y -- true \"label\"\n",
    "        epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "        \n",
    "    Returns:\n",
    "        difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        thetaplus = np.copy(parameters_values)\n",
    "        thetaplus[i][0] = thetaplus[i][0]+epsilon\n",
    "        AL_plus, _= L_model_forward(X, vector_to_dictionary(thetaplus))\n",
    "        J_plus[i] = compute_cost(AL_plus, Y)\n",
    "        \n",
    "        thetaminus = np.copy(parameters_values)\n",
    "        thetaminus[i][0] = thetaminus[i][0]-epsilon\n",
    "        AL_minus, _= L_model_forward(X, vector_to_dictionary(thetaminus))\n",
    "        J_minus[i] = compute_cost(AL_minus, Y)\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i]-J_minus[i])/2/epsilon\n",
    "    \n",
    "    numerator = np.linalg.norm(grad-gradapprox)\n",
    "    denominator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print(\"There is a mistake in the backward propagation! difference = \"+str(difference))\n",
    "    else:\n",
    "        print(\"The backward propagation works perfectly fine! difference = \"+str(difference))\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 - Update Parameters\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        parameters -- python dictionary containing parameters\n",
    "        grads -- python dictionary containing gradients, output of L_model_backward\n",
    "        \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 - Two-layer neural network\n",
    "- *LINEAR -> RELU -> LINEAR -> SIGMOID*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- input data, of shape (n_x, number of examples)\n",
    "        Y -- true \"label\" vector (containing 0 if cat, 1 if not-cat), of shape (1, number of examples)\n",
    "        layer_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_cost -- If set to True, this will print the cost every 100 iterations\n",
    "        \n",
    "    Returns:\n",
    "        parameters -- a dictionary containing W1, W2, b1 and b2\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the cost\n",
    "    m = X.shape[1] # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "        \n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('costs')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\"+str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 - L-layer Neural Network\n",
    "- *[LINEAR -> RELU]X(L-1) -> LINEAR -> SIGMOID*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- data, numpy array of shape (number of examples, num_px*num_px*3)\n",
    "        Y -- true \"label\" vector\n",
    "        layer_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "        num_iterations -- number of iterations of the optimization loop\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_cost -- If set to True, this will print the cost every 100 iterations\n",
    "        \n",
    "    Returns:\n",
    "        parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = [] # keep track of cost\n",
    "    #parameters = initialize_parameters_deep(layers_dims)\n",
    "    parameters = initialize_parameters_he(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # gradient checking\n",
    "        difference = gradient_check_n(parameters, grads, X, Y)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('costs')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\"+str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 - Train the model\n",
    "For developing the model, use *x_dev*. For training the model, use *x_train*.\n",
    "This step will take some time which depends on the performance of the CPU/GPU.\n",
    "\n",
    "#### a) 2-layer model\n",
    "This will take about 26mins with the dev-set on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "n_x = x_dev.shape[0] # num_px*num_px*3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6910442484436397\n",
      "Cost after iteration 100: 0.6441901393552899\n",
      "Cost after iteration 200: 0.6182099860089636\n",
      "Cost after iteration 300: 0.5791538276014184\n",
      "Cost after iteration 400: 0.5780064915470419\n",
      "Cost after iteration 500: 0.5629681385993701\n",
      "Cost after iteration 600: 0.5487578589331287\n",
      "Cost after iteration 700: 0.5361814742381072\n",
      "Cost after iteration 800: 0.5235416590093201\n",
      "Cost after iteration 900: 0.5107184671019254\n",
      "Cost after iteration 1000: 0.5010717881092689\n",
      "Cost after iteration 1100: 0.49059311005594014\n",
      "Cost after iteration 1200: 0.480290102407819\n",
      "Cost after iteration 1300: 0.4685421938956865\n",
      "Cost after iteration 1400: 0.4584976020737072\n",
      "Cost after iteration 1500: 0.45298961002805627\n",
      "Cost after iteration 1600: 0.4417210413154317\n",
      "Cost after iteration 1700: 0.4277441761299085\n",
      "Cost after iteration 1800: 0.4222766936044148\n",
      "Cost after iteration 1900: 0.4138735713092359\n",
      "Cost after iteration 2000: 0.4134284055368738\n",
      "Cost after iteration 2100: 0.4076992748347584\n",
      "Cost after iteration 2200: 0.3963375675473676\n",
      "Cost after iteration 2300: 0.39202138241914036\n",
      "Cost after iteration 2400: 0.38244670202448156\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1489.1206624507904 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = tm.time()\n",
    "parameters_2L = two_layer_model(x_dev, y_dev, layers_dims=(n_x, n_h, n_y), num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 0.693049753... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.04855478... </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) L-layer model\n",
    "This will take about 33mins with the dev-set on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "layers_dims = [x_dev.shape[0], 20, 7, 5, 1] # 4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4 W4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'W0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-f8e5cae12e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparameters_NL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-a89a46f9574c>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# gradient checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_check_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-4cbbfc1900ac>\u001b[0m in \u001b[0;36mgradient_check_n\u001b[0;34m(parameters, gradients, X, Y, epsilon)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mthetaplus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mthetaplus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthetaplus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mAL_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_to_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthetaplus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mJ_plus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-02205ff7ce60>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W0'"
     ]
    }
   ],
   "source": [
    "start_time = tm.time()\n",
    "parameters_NL = L_layer_model(x_dev, y_dev, layers_dims, num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan=\"2\">Random Initialization</th><th colspan=\"2\">He Initialization</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 0.771749 </td>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 1.208125 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.092878 </td>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.505814 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 -- Calculate the accurancy of the predicting results\n",
    "Predict the results and the accuracy rate.\n",
    "$$\\hat{Y} = A = \\sigma(w^T X + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- data set of examples\n",
    "        parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "        p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    \n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "            \n",
    "    print(\"Accuracy: \"+str(np.sum((p==y)/m)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8233918128654969\n"
     ]
    }
   ],
   "source": [
    "pred_2L_dev = predict(x_dev, y_dev, parameters_2L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 1.0 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9087719298245611\n"
     ]
    }
   ],
   "source": [
    "pred_NL_dev = predict(x_dev, y_dev, parameters_NL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Train Accuracy**</td>\n",
    "        <td> 0.985645933014 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6557377049180327\n"
     ]
    }
   ],
   "source": [
    "pred_2L_test_d = predict(x_test, y_test, parameters_2L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 0.72 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.563231850117096\n"
     ]
    }
   ],
   "source": [
    "pred_NL_test_d = predict(x_test, y_test, parameters_NL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** (for verification):\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 0.8 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Train the model with training data\n",
    "### 5.1 2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "n_x = x_train.shape[0] # 128*128*3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = tm.time()\n",
    "parameters_2L_t = two_layer_model(x_train, y_train, layers_dims=(n_x, n_h, n_y), num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 L-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "layers_dims = [x_train.shape[0], 20, 7, 5, 1] # 4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = tm.time()\n",
    "parameters_NL_t = L_layer_model(x_train, y_train, layers_dims, num_iterations=2500, print_cost=True)\n",
    "print(\"--- %s seconds ---\" %(tm.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Accuracy rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2L_train = predict(x_train, y_train, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NL_train = predict(x_train, y_train, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Verify with testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2L_test = predict(x_test, y_test, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NL_test = predict(x_train, y_train, parameters_2L_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
