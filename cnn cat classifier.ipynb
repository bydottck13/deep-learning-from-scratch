{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Deep Learning from scratch\n",
    "## Convolutional Neural Networks for a cat classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from dnn_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Networks\n",
    "### 2.1 - Zero-Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "        pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "        X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values=(0,))\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Single step of convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "        W -- weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "        b -- bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "        \n",
    "    Returns:\n",
    "        Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    Z = np.sum(s)\n",
    "    Z = np.add(Z, np.float(b))\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Convolutional Neural Networks - Forward pass\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1$$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1$$\n",
    "$$ n_C = \\text{number of filiters used in the convolution}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        A_prev -- output of activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        W -- weight, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        b -- biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "    \n",
    "    Returns:\n",
    "        Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    n_H = int((n_H_prev-f+2*pad)/stride)+1\n",
    "    n_W = int((n_W_prev-f+2*pad)/stride)+1\n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    vert_start = stride*h\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                    \n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Pooling layer\n",
    "#### Forward Pooling\n",
    "$$n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor + 1$$\n",
    "$$n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor + 1$$\n",
    "$$n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        A_prev -- input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "        mode -- the pooling mode\n",
    "        \n",
    "    Returns:\n",
    "        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    n_H = int(1+(n_H_prev-f)/stride)\n",
    "    n_W = int(1+(n_W_prev-f)/stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    vert_start = stride*h\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i,vert_start:vert_end, horiz_start:horiz_end,c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Backpropagation in convolutional neural networks\n",
    "#### a) Convolutional layer backward pass\n",
    "- Computing dA:\n",
    "$$ dA += \\sum\\limits_{h = 0}^{n_{H}}{\\sum\\limits_{w = 0}^{n_{W}}{W_{c} \\times dZ_{hw}} }$$\n",
    "\n",
    "- Computing dW:\n",
    "$$ dW_{c} += \\sum\\limits_{h = 0}^{n_{H}}{ \\sum\\limits_{w = 0}^{n_{W}}{a_{slice} \\times dZ_{hw}} }$$\n",
    "\n",
    "- Computing db:\n",
    "$$ db = \\sum\\limits_{h}{ \\sum\\limits_{w}{ dZ_{hw} }} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "        \n",
    "    Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW -- gradient of the cost with respect to the weights of the conv layer (W),\n",
    "                numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        db -- gradient of the cost with respect to the biases of the conv layer (b),\n",
    "                numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:] += W[:,:,:,c]*dZ[i,h,w,c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i,h,w,c]\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c]\n",
    "        \n",
    "        dA_prev[i,:,:,:] = da_prev_pad[pad:-pad,pad:-pad,:]\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Pooling layer - backward pass\n",
    "- Max pooling - backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "        mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Average pooling - backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dz -- input scalar\n",
    "        shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "        a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    (n_H, n_W) = shape\n",
    "    average = dz / (n_H*n_W)\n",
    "    a = np.ones(shape)*average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Putting it together: Pooling backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "        cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters\n",
    "        mode -- the pooling mode\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    (A_prev, hparameters) = cache\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i,h,w,c])\n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i,h,w,c]\n",
    "                        shape = (f, f)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end,c] += distribute_value(da, shape)\n",
    "        \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Application: TensorFlow model\n",
    "### 3.1 - Create placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        n_H0 -- scalar, height of an input image\n",
    "        n_W0 -- scalar, width of an input image\n",
    "        n_C0 -- scalar, number of channels of the input\n",
    "        n_y -- scalar, number of classes\n",
    "    \n",
    "    Returns:\n",
    "        X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "        Y -- placeholder for the data input, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_H0, n_W0, n_C0))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_y))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \n",
    "    parameters = {\"W1\":W1,\n",
    "                  \"W2\":W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Forward propagation\n",
    " - CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"W2\"\n",
    "                    the shapes are given in initialize_parameters\n",
    "    \n",
    "    Returns:\n",
    "        Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[1,8,8,1], strides=[1,8,8,1], padding='SAME')\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding='SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,4,4,1], strides=[1,4,4,1], padding='SAME')\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn=None)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_binary(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "        parameters -- python dictionary containing parameters \"W1\", \"W2\"\n",
    "                    the shapes are given in initialize_parameters\n",
    "    \n",
    "    Returns:\n",
    "        Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[1,8,8,1], strides=[1,8,8,1], padding='SAME')\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1,1,1,1], padding='SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,4,4,1], strides=[1,4,4,1], padding='SAME')\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn=None)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "        Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "        \n",
    "    Returns:\n",
    "        cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Model\n",
    " - CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "         num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        X_train -- training set, of shape (None, 64, 64, 3)\n",
    "        Y_train -- training set, of shape (None, n_y = 6)\n",
    "        X_test -- test set, of shape (None, 64, 64, 3)\n",
    "        Y_test -- test set, of shape (None, n_y = 6)\n",
    "        learning_rate -- learning rate of the optimization\n",
    "        num_epochs -- number of epochs of the optimization loop\n",
    "        minibatch_size -- size of a minibatch\n",
    "        print_cost -- True to print the cost every 100 epochs\n",
    "        \n",
    "    Returns:\n",
    "        train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "        test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "        parameters -- parameters learnt by the model. They can then be used to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed = 3\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    parameters = initialize_parameters()\n",
    "    #Z3 = forward_propagation(X, parameters)\n",
    "    Z3 = forward_propagation_binary(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m/minibatch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _, temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            \n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print(\"Cost after epoch %i: %f\" %(epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epochs (per fives)')\n",
    "        plt.title(\"Learning rate =\"+ str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "        \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train's shape=(1080, 64, 64, 3)\n",
      "Y_train's shape=(1080, 6)\n",
      "X_test's shape=(120, 64, 64, 3)\n",
      "Y_test's shape=(120, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load verifying datasets\n",
    "X_train = np.load(\"datasets/X_train_tf.npy\")\n",
    "Y_train = np.load(\"datasets/Y_train_tf.npy\")\n",
    "X_test = np.load(\"datasets/X_test_tf.npy\")\n",
    "Y_test = np.load(\"datasets/Y_test_tf.npy\")\n",
    "print(\"X_train's shape=\"+str(X_train.shape))\n",
    "print(\"Y_train's shape=\"+str(Y_train.shape))\n",
    "print(\"X_test's shape=\"+str(X_test.shape))\n",
    "print(\"Y_test's shape=\"+str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0, 1.917929\n",
      "Cost after epoch 5, 1.506757\n",
      "Cost after epoch 10, 0.955359\n",
      "Cost after epoch 15, 0.845802\n",
      "Cost after epoch 20, 0.701174\n",
      "Cost after epoch 25, 0.571977\n",
      "Cost after epoch 30, 0.518435\n",
      "Cost after epoch 35, 0.495806\n",
      "Cost after epoch 40, 0.429827\n",
      "Cost after epoch 45, 0.407291\n",
      "Cost after epoch 50, 0.366394\n",
      "Cost after epoch 55, 0.376922\n",
      "Cost after epoch 60, 0.299491\n",
      "Cost after epoch 65, 0.338870\n",
      "Cost after epoch 70, 0.316400\n",
      "Cost after epoch 75, 0.310413\n",
      "Cost after epoch 80, 0.249549\n",
      "Cost after epoch 85, 0.243457\n",
      "Cost after epoch 90, 0.200031\n",
      "Cost after epoch 95, 0.175452\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "Train Accuracy: 0.94074076\n",
      "Test Accuracy: 0.78333336\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape: (4276, 49152)\n",
      "Y's shape: (4276, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load my datasets\n",
    "X = np.load('datasets/X.npy').T\n",
    "Y = np.load('datasets/Y.npy')\n",
    "print(\"X's shape: \"+str(X.shape))\n",
    "print(\"Y's shape: \"+str(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffled_X's shape: (4276, 49152)\n",
      "shuffled_Y's shape: (4276, 1)\n"
     ]
    }
   ],
   "source": [
    "# Re-shuffle X and y_array\n",
    "permutation = list(np.random.permutation(X.shape[0]))\n",
    "shuffled_X = X[permutation,:]\n",
    "shuffled_Y = Y[permutation,:]\n",
    "\n",
    "print(\"shuffled_X's shape: \"+str(shuffled_X.shape))\n",
    "print(\"shuffled_Y's shape: \"+str(shuffled_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 3848\n",
      "Number of developing examples: 214\n",
      "Number of testing examples: 214\n"
     ]
    }
   ],
   "source": [
    "m_test = np.rint(shuffled_X.shape[0]*0.05).astype(int)\n",
    "m_dev = m_test\n",
    "m_train = shuffled_X.shape[0]-m_test-m_dev\n",
    "print(\"Number of training examples: \" + str(m_train))\n",
    "print(\"Number of developing examples: \" + str(m_dev))\n",
    "print(\"Number of testing examples: \" + str(m_test))\n",
    "\n",
    "assert(m_test+m_dev+m_train==shuffled_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape: (3848, 128, 128, 3)\n",
      "y_train's shape: (3848, 1)\n",
      "y_test's shape: (214, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = shuffled_X[0:m_train,:].reshape((m_train, 128, 128, 3))\n",
    "y_train = shuffled_Y[0:m_train,:]\n",
    "x_dev = shuffled_X[m_train:m_train+m_dev,:].reshape((m_dev, 128, 128, 3))\n",
    "y_dev = shuffled_Y[m_train:m_train+m_dev,:]\n",
    "x_test = shuffled_X[m_train+m_dev:,:].reshape((m_test, 128, 128, 3))\n",
    "y_test = shuffled_Y[m_train+m_dev:,:]\n",
    "\n",
    "print(\"x_train's shape: \"+str(x_train.shape))\n",
    "print(\"y_train's shape: \"+str(y_train.shape))\n",
    "print(\"y_test's shape: \"+str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[64,2] labels_size=[64,1]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape, Reshape_1)]]\n\nCaused by op 'SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-6f501a653c0b>\", line 1, in <module>\n    _, _, parameters = model(x_dev, y_dev, x_test, y_test)\n  File \"<ipython-input-59-2edfb04bcb0c>\", line 31, in model\n    cost = compute_cost(Z3, Y)\n  File \"<ipython-input-5-176bfee10c0a>\", line 11, in compute_cost\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1594, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2380, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[64,2] labels_size=[64,1]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape, Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[64,2] labels_size=[64,1]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape, Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-6f501a653c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-2edfb04bcb0c>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemp_cost\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_minibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[64,2] labels_size=[64,1]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape, Reshape_1)]]\n\nCaused by op 'SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-60-6f501a653c0b>\", line 1, in <module>\n    _, _, parameters = model(x_dev, y_dev, x_test, y_test)\n  File \"<ipython-input-59-2edfb04bcb0c>\", line 31, in model\n    cost = compute_cost(Z3, Y)\n  File \"<ipython-input-5-176bfee10c0a>\", line 11, in compute_cost\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1594, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2380, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[64,2] labels_size=[64,1]\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape, Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(x_dev, y_dev, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
